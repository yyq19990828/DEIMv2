这个 `policy` 配置定义了**数据增强操作的动态调度策略**,用于在训练的不同阶段控制特定数据增强操作的启用/禁用。具体解释如下:

## 配置参数

```yaml
policy:
  epoch: [4, 29, 50]   # 三个关键epoch节点
  ops: ['Mosaic', 'RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']
```

## 训练阶段划分

根据 [`container.py:81-99`](engine/data/transforms/container.py:81-99) 的实现逻辑,这个配置将训练过程分为**4个阶段**:

### 第1阶段: epoch < 4 (预热阶段)
- **禁用** `policy.ops` 中列出的所有增强操作
- 只使用基础的数据增强(如 Resize, Flip 等)
- 目的: 让模型在训练初期使用较简单的数据,快速收敛

### 第2阶段: 4 ≤ epoch < 29 (强增强阶段)
- **启用** Mosaic、RandomPhotometricDistort、RandomZoomOut、RandomIoUCrop
- Mosaic 以 `mosaic_prob: 0.5` 的概率应用
- **互斥规则**: 当使用 Mosaic 时,会跳过 RandomZoomOut 和 RandomIoUCrop (见第96行)
- 目的: 使用强数据增强提升模型泛化能力

### 第3阶段: 29 ≤ epoch < 50 (过渡阶段)
- **禁用** Mosaic (不再随机应用)
- 仍可能使用 RandomPhotometricDistort、RandomZoomOut、RandomIoUCrop
- 目的: 逐步减弱增强强度

### 第4阶段: epoch ≥ 50 (微调阶段)
- **禁用** 所有 `policy.ops` 中的增强操作
- 只保留基础增强
- 目的: 让模型在接近真实数据分布的情况下微调,提升最终精度

## 核心逻辑

从 [`container.py:82-99`](engine/data/transforms/container.py:82-99) 可以看到:
- 第1阶段和第4阶段跳过 policy_ops 中的所有操作
- 第2阶段使用 Mosaic 时,会跳过 ZoomOut/IoUCrop(避免冲突)
- 这种渐进式的增强策略有助于训练稳定性和最终性能

这是目标检测训练中常用的**渐进式数据增强策略**,在不同训练阶段使用不同强度的数据增强。

这个 `collate_fn` 配置定义了**批次级别的数据增强策略**,在数据加载器的 collate 阶段应用。主要包含三种增强技术和多尺度训练策略:

## 配置参数详解

```yaml
collate_fn:
  # Mixup 选项
  mixup_prob: 0.5              # Mixup 应用概率
  mixup_epochs: [4, 29]        # Mixup 生效的 epoch 范围
  stop_epoch: 50               # 多尺度训练停止的 epoch
  
  # CopyBlend 选项
  copyblend_prob: 0.5          # CopyBlend 应用概率
  copyblend_epochs: [4, 50]    # CopyBlend 生效的 epoch 范围
  area_threshold: 100          # 目标最小面积阈值(像素)
  num_objects: 3               # 每次混合的目标数量
  with_expand: True            # 是否扩展目标周围区域
  expand_ratios: [0.1, 0.25]   # 扩展比例范围
  
  # 其他参数
  ema_restart_decay: 0.9999    # EMA 衰减系数
  base_size_repeat: 4          # 基础尺寸重复次数
```

---

## 1. Mixup 增强 (epoch 4-29)

**实现位置**: [`dataloader.py:153-196`](engine/data/dataloader.py:153-196)

**工作原理**:
- 在 batch 内将图像与其循环移位后的图像进行线性混合
- 混合比例 β ∈ [0.45, 0.55] 随机生成
- 公式: `mixed_image = β × image_i + (1-β) × image_{i+1}`

**目标处理**:
- 合并两张图像的所有边界框和标签
- 为每个目标添加 `mixup` 权重字段:
  - 原图目标权重: β
  - 移位图目标权重: 1-β

**生效条件**:
- `random.random() < 0.5` (50% 概率)
- `4 ≤ epoch < 29`

---

## 2. CopyBlend 增强 (epoch 4-50)

**实现位置**: [`dataloader.py:197-345`](engine/data/dataloader.py:197-345)

**工作原理**:
这是一种**目标级别的复制混合**技术,从 batch 中的其他图像复制目标到当前图像。

### 核心步骤:

#### 步骤1: 构建目标池 (第206-228行)
```python
# 从整个 batch 收集所有有效目标
for 每张图像:
    筛选面积 ≥ 100 像素的目标
    加入 objects_pool
```

#### 步骤2: 随机选择目标 (第235-243行)
- 每张图像随机选择 **3个目标** 进行混合
- 从 objects_pool 中随机采样

#### 步骤3: 目标复制与混合 (第250-310行)
对每个选中的目标:
1. **提取源目标区域** (第260-270行)
   - 从源图像中裁剪目标的边界框区域
   
2. **可选扩展** (第290-301行,`with_expand=True`)
   - 扩展比例 α ∈ [0.1, 0.25]
   - 扩展目标周围区域,包含更多上下文信息
   - 处理边界溢出情况

3. **混合到目标图像** (第303-309行)
   - 随机选择粘贴位置
   - 使用 β 比例混合: `blended = β × 原图 + (1-β) × 复制区域`
   - β ∈ [0.45, 0.55]

4. **更新目标标注** (第311-325行)
   - 添加新的边界框(归一化坐标)
   - 添加对应的类别标签
   - 设置 mixup 权重为 `1-β`

**生效条件**:
- `random.random() < 0.5` (50% 概率)
- `4 ≤ epoch < 50`
- **与 Mixup 互斥**: 同一 batch 只应用其中一种

**关键参数作用**:
- `area_threshold: 100`: 过滤太小的目标,避免复制低质量样本
- `num_objects: 3`: 控制增强强度,避免过度混合
- `with_expand: True`: 包含目标周围上下文,提升混合质量
- `expand_ratios: [0.1, 0.25]`: 扩展10%-25%的目标尺寸

---

## 3. 多尺度训练 (epoch < 50)

**实现位置**: [`dataloader.py:354-364`](engine/data/dataloader.py:354-364)

**工作原理**:
- 使用 [`generate_scales()`](engine/data/dataloader.py:87-92) 生成尺度序列
- 基于 `base_size=640` 和 `base_size_repeat=4`
- 每个 batch 随机选择一个尺度进行 resize

**尺度生成逻辑**:
```python
# 假设 base_size=640, base_size_repeat=4
scale_repeat = (640 - int(640*0.75/32)*32) // 32  # 计算变化范围
scales = [
    480, 512, 544, 576, 608,  # 0.75x ~ 1.0x (递增)
    640, 640, 640, 640,        # 1.0x (重复4次,增加基础尺度权重)
    672, 704, 736, 768, 800    # 1.0x ~ 1.25x (递减)
]
```

**停止条件**: `epoch ≥ 50` 后固定使用 640×640

---

## 4. EMA 参数

`ema_restart_decay: 0.9999` 用于模型权重的指数移动平均,与数据增强无直接关系,但影响训练稳定性。

---

## 训练阶段总结

| Epoch 范围 | Mixup | CopyBlend | 多尺度 |
|-----------|-------|-----------|--------|
| 0-3       | ❌    | ❌        | ✅     |
| 4-28      | ✅ 50% | ✅ 50% (互斥) | ✅ |
| 29-49     | ❌    | ✅ 50%    | ✅     |
| 50+       | ❌    | ❌        | ❌     |

这种**渐进式减弱增强**的策略有助于:
1. 早期阶段增强数据多样性
2. 后期阶段让模型在接近真实分布的数据上微调
3. 提升最终检测精度和泛化能力


这是一个很好的问题!数据增强分别写在 `ops` 和 `collate_fn` 两个地方,是因为它们在**数据处理流程的不同阶段**执行,有着本质的区别:

---

## 数据处理流程

```
单样本加载 → ops增强 → 组batch → collate_fn增强 → 送入模型
   ↓              ↓           ↓            ↓
Dataset      Transform    DataLoader   Collate
(单个样本)    (单样本级)    (收集)      (批次级)
```

---

## 1. ops (Transform 阶段) - 单样本增强

**执行位置**: [`engine/data/transforms/container.py`](engine/data/transforms/container.py)

**处理对象**: **单个样本** (一张图像 + 标注)

**增强操作**:
```yaml
ops:
  - Mosaic              # 需要4张图像拼接
  - RandomPhotometricDistort  # 颜色抖动
  - RandomZoomOut       # 随机缩小
  - RandomIoUCrop       # 随机裁剪
  - RandomHorizontalFlip  # 水平翻转
  - Resize              # 调整大小
```

**特点**:
- **独立处理**: 每个样本独立增强,互不影响
- **几何变换**: 主要是图像级的几何和颜色变换
- **Mosaic 特殊性**: 虽然需要4张图,但在 Dataset 的 `__getitem__` 中实现,仍属于单样本构建过程

**为什么在这里**:
- 这些操作需要**原始图像的完整信息**
- 需要在**组batch之前**完成,因为每个样本的尺寸可能不同
- 例如 Resize 必须在 collate 前完成,否则无法堆叠成 tensor

---

## 2. collate_fn (Collate 阶段) - 批次增强

**执行位置**: [`engine/data/dataloader.py`](engine/data/dataloader.py)

**处理对象**: **整个 batch** (多张图像 + 多个标注)

**增强操作**:
```yaml
collate_fn:
  - Mixup          # 需要batch内的图像对
  - CopyBlend      # 需要batch内的目标池
  - MultiScale     # 需要统一调整batch尺寸
```

**特点**:
- **批次依赖**: 需要访问 batch 内的**多个样本**
- **样本间交互**: 在不同样本之间混合信息
- **统一处理**: 对整个 batch 进行统一操作

**为什么在这里**:

### 原因1: Mixup 需要样本对
```python
# 需要访问 batch 内的其他图像
images = images.roll(shifts=1, dims=0).mul_(1-β).add_(images.mul(β))
#        ↑ 循环移位,获取下一张图像
```
- 在 Transform 阶段无法实现,因为只能访问单个样本
- 必须等到 batch 组装完成后才能进行

### 原因2: CopyBlend 需要目标池
```python
# 从整个 batch 收集目标
for i in range(len(images)):
    # 收集所有图像的有效目标到 objects_pool
    objects_pool['boxes'].append(...)
    
# 然后从池中随机选择目标复制到其他图像
```
- 需要**跨样本**访问目标信息
- Transform 阶段无法实现跨样本操作

### 原因3: MultiScale 需要统一尺寸
```python
# 整个 batch 必须使用相同尺寸
sz = random.choice(self.scales)
images = F.interpolate(images, size=sz)
#        ↑ 对整个 batch tensor 操作
```
- batch 内所有图像必须有相同的 shape 才能堆叠成 4D tensor
- 虽然 Transform 中有 Resize,但 collate 阶段的多尺度是**动态的、随机的**

---

## 关键区别总结

| 维度 | ops (Transform) | collate_fn (Collate) |
|------|----------------|---------------------|
| **处理单位** | 单个样本 | 整个 batch |
| **数据形式** | PIL Image / dict | Tensor batch |
| **样本访问** | 只能访问当前样本 | 可访问 batch 内所有样本 |
| **典型操作** | 裁剪、翻转、颜色变换 | Mixup、CopyBlend、多尺度 |
| **执行时机** | Dataset.\_\_getitem\_\_() | DataLoader.collate_fn() |
| **并行性** | 可多进程并行 | 在主进程执行 |

---

## 为什么不能互换?

### ❌ 不能把 Mixup 放到 Transform
```python
# Transform 阶段无法实现
def transform(self, sample):
    # 问题: 如何获取"下一个样本"?
    next_sample = ???  # 无法访问!
    mixed = sample * β + next_sample * (1-β)
```

### ❌ 不能把 Resize 放到 Collate
```python
# Collate 阶段会出问题
def collate_fn(items):
    # items 中每个图像尺寸不同
    images = [x[0] for x in items]  
    # 问题: 无法堆叠成 tensor!
    batch = torch.stack(images)  # ❌ RuntimeError: stack expects each tensor to be equal size
```

---

## 设计原则

这种分离设计遵循了**数据处理的自然流程**:

1. **Transform**: 准备单个样本,使其满足基本要求(尺寸统一、格式正确)
2. **Collate**: 利用 batch 信息进行高级增强,增加样本多样性

这样的设计既保证了**灵活性**(可以独立配置两个阶段),又保证了**效率**(Transform 可并行,Collate 利用 batch 信息)。